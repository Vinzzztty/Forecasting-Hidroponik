# -*- coding: utf-8 -*-
"""V3 [Forecasting] Model Simulasi Otomatisasi HIdroponik mas dengan model Prophet dan Arima

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OkThf_2B26qGtFnej3b3eldOihHizNsg

# 1. Business Understanding
- Mengembangkan Model SImulasi Otomatisasi HIdroponik mas dengan model Prophet dan Arima
- Membandingkan 2 Metode Algoritma Prophet dan ARIMA mana yang terbaik dan efektif.
- Mengembangkan Aplikasi Simulasi Hidroponik (HydroSim) untuk mlakukan simulasi otomasisasi hidroponik tanaman selada dengan menggunakan model terbaik

# 2. Data Understanding
"""

import pandas as pd
from sklearn.model_selection import KFold, train_test_split
from sklearn.metrics import mean_squared_error
from prophet import Prophet
from statsmodels.tsa.arima.model import ARIMA
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.stattools import adfuller, acf, pacf
from math import sqrt
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin
from tabulate import tabulate
from sklearn.metrics import confusion_matrix

"""## Load Dataset"""

df_train = pd.read_csv("/content/DataFieldFULLSIOHITrainFULLPattern01072024.csv")
df_test = pd.read_csv("/content/DataFieldFULLSIOHITest01072024.csv")

print(len(df_train))
print(len(df_test))

df_train.head()

df_train.tail()

"""### EDA"""

df_train.columns

df_train.info()

# Check missing data

missing_data = pd.DataFrame({'total_missing': df_train.isnull().sum(),
                             'perc_missing': (df_train.isnull().sum()/2634)*100})

missing_data

df_train.describe()

df_train = df_train.drop(columns=['Label'])
df_test = df_test.drop(columns=['Label'])

# Correlation Matrix
cor_matrix = df_train.corr()
cor_matrix

# Heatmap of the correlation matrix
plt.figure(figsize=(15, 5))
sns.heatmap(cor_matrix, annot=True, cmap='YlGnBu', fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix")
plt.show()

"""### Visualisasi"""

# Plotting histograms
plt.figure(figsize=(15, 10))

# Histogram for 'temperature'
plt.subplot(3, 3, 1)
sns.histplot(df_train['temperature'], bins=20, kde=True, color='skyblue')
plt.title('Histogram of Temperature')
plt.xlabel('Temperature')
plt.ylabel('Frequency')

# Histogram for 'humidity'
plt.subplot(3, 3, 2)
sns.histplot(df_train['humidity'], bins=20, kde=True, color='salmon')
plt.title('Histogram of Humidity')
plt.xlabel('Humidity')
plt.ylabel('Frequency')

# Histogram for 'light'
plt.subplot(3, 3, 3)
sns.histplot(df_train['light'], bins=20, kde=True, color='green')
plt.title('Histogram of Light')
plt.xlabel('Light')
plt.ylabel('Frequency')

# Histogram for 'pH'
plt.subplot(3, 3, 4)
sns.histplot(df_train['pH'], bins=20, kde=True, color='purple')
plt.title('Histogram of pH')
plt.xlabel('pH')
plt.ylabel('Frequency')

# Histogram for 'EC'
plt.subplot(3, 3, 5)
sns.histplot(df_train['EC'], bins=20, kde=True, color='orange')
plt.title('Histogram of EC')
plt.xlabel('EC')
plt.ylabel('Frequency')

# Histogram for 'TDS'
plt.subplot(3, 3, 6)
sns.histplot(df_train['TDS'], bins=20, kde=True, color='brown')
plt.title('Histogram of TDS')
plt.xlabel('TDS')
plt.ylabel('Frequency')

# Histogram for 'WaterTemp'
plt.subplot(3, 3, 7)
sns.histplot(df_train['WaterTemp'], bins=20, kde=True, color='pink')
plt.title('Histogram of WaterTemp')
plt.xlabel('WaterTemp')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

"""#### Visualisasi Terhadap Hari"""

# Group by 'day' and calculate the mean of 'LeafCount'
average_leaf_count = df_train.groupby('day')['LeafCount'].mean().reset_index()

# Plotting the data
plt.figure(figsize=(10, 6))
plt.plot(average_leaf_count['day'], average_leaf_count['LeafCount'], marker='o', linestyle='-', color='b')

# Adding titles and labels
plt.title('Average Leaf Count Over Days')
plt.xlabel('Day')
plt.ylabel('Average Leaf Count')

# Display the plot
plt.grid(True)
plt.show()

# Group by 'day' and calculate the mean of 'EC' and 'TDS'
average_values = df_train.groupby('day')[['EC', 'TDS']].mean().reset_index()

# Creating the subplots
fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)

# Plotting average EC vs day
axs[0].plot(average_values['day'], average_values['EC'], marker='o', linestyle='-', color='r')
axs[0].set_title('Average EC vs Day')
axs[0].set_ylabel('Average EC')
axs[0].grid(True)

# Plotting average TDS vs day
axs[1].plot(average_values['day'], average_values['TDS'], marker='o', linestyle='-', color='g')
axs[1].set_title('Average TDS vs Day')
axs[1].set_xlabel('Day')
axs[1].set_ylabel('Average TDS')
axs[1].grid(True)

# Displaying the plots
plt.tight_layout()
plt.show()

"""#### Visualisasi dengan Jumlah Daun"""

# Plotting EC vs TDS with LeafCount as marker size
plt.figure(figsize=(10, 6))
plt.scatter(df_train['EC'], df_train['TDS'], s=df_train['LeafCount']*10, alpha=0.5, c='blue', label='LeafCount')
plt.title('EC and TDS vs LeafCount')
plt.xlabel('EC')
plt.ylabel('TDS')
plt.colorbar(label='LeafCount')
plt.legend()

plt.grid(True)
plt.show()

"""# 3. Data Preparation"""

df_train.info()

df_train_copy = df_train.copy()
df_test_copy = df_test.copy()

df_train_copy['time']
df_test_copy['time']

# Tambahkan "0" di depan nilai yang hanya satu digit di bagian jam
df_train_copy['time'] = df_train_copy['time'].apply(lambda x: '{:.2f}'.format(x))
df_test_copy['time'] = df_test_copy['time'].apply(lambda x: '{:.2f}'.format(x))

# Konversi 'time' ke format waktu yang benar
df_train_copy['time'] = pd.to_datetime(df_train_copy['time'], format='%H.%M').dt.time
df_test_copy['time'] = pd.to_datetime(df_test_copy['time'], format='%H.%M').dt.time

df_train_copy

df_test_copy

df_train_prep = df_train_copy.copy()
df_test_prep = df_test_copy.copy()

df_train_prep = df_train_prep[['day', 'time', 'LeafCount']]
df_test_prep = df_test_prep[['day', 'time', 'LeafCount']]

# Tentukan tanggal awal
start_date = pd.to_datetime('2024-07-01')

# Ubah kolom 'time' menjadi string
df_train_prep['time'] = df_train_prep['time'].astype(str)
df_test_prep['time'] = df_test_prep['time'].astype(str)

# Buat kolom 'datetime' dengan menambahkan 'day' ke tanggal awal dan menggabungkan dengan 'time'
df_train_prep['datetime'] = df_train_prep.apply(lambda row: start_date + pd.Timedelta(days=row['day'] - 1) + pd.to_timedelta(row['time']), axis=1)
df_test_prep['datetime'] = df_test_prep.apply(lambda row: start_date + pd.Timedelta(days=row['day'] - 1) + pd.to_timedelta(row['time']), axis=1)

print(df_train_prep.head())

"""### Data Pre-Processing"""

print(df_test_prep.head())

df_test_prep.info()

# Convert to correct data types
df_train_prep['day'] = df_train_prep['day'].astype(int)
df_train_prep['time'] = df_train_prep['time'].astype(str)
df_train_prep['LeafCount'] = df_train_prep['LeafCount'].astype(int)

# Convert to correct data types
df_test_prep['day'] = df_test_prep['day'].astype(int)
df_test_prep['time'] = df_test_prep['time'].astype(str)
df_test_prep['LeafCount'] = df_test_prep['LeafCount'].astype(int)

df_train_prep

# Convert to correct data types
df_train_copy['day'] = df_train_copy['day'].astype(int)
df_train_copy['time'] = df_train_copy['time'].astype(str)
df_train_copy['LeafCount'] = df_train_copy['LeafCount'].astype(int)

df_test_copy['day'] = df_test_copy['day'].astype(int)
df_test_copy['time'] = df_test_copy['time'].astype(str)
df_test_copy['LeafCount'] = df_test_copy['LeafCount'].astype(int)

# Merge the 'datetime' column from df_prep into df_copy
df_train_model = pd.merge(df_train_copy, df_train_prep[['day', 'time', 'LeafCount', 'datetime']], on=['day', 'time', 'LeafCount'], how='inner')
df_test_model = pd.merge(df_test_copy, df_test_prep[['day', 'time', 'LeafCount', 'datetime']], on=['day', 'time', 'LeafCount'], how='inner')

# Debug: Check the result of the merge
print("df_model after merge:")
print(df_train_model.head())

df_train_model

df_test_model

df_test_model.duplicated().sum()

df_train_model.duplicated().sum()

# Drop duplicates
df_train_model = df_train_model.drop_duplicates(subset=['day', 'time', 'LeafCount'])
df_test_model = df_test_model.drop_duplicates(subset=['day', 'time', 'LeafCount'])

# Set the 'datetime' column as the index and sort it
df_train_model.set_index('datetime', inplace=True)
df_train_model = df_train_model.sort_index()

# Set the 'datetime' column as the index and sort it
df_test_model.set_index('datetime', inplace=True)
df_test_model = df_test_model.sort_index()

# Ensure that the data is sorted by datetime
df_train_model = df_train_model.sort_index()
df_test_model = df_test_model.sort_index()

# save dataframe to csv
df_train_model.to_csv('dataset_train_final.csv')
df_test_model.to_csv('dataset_test_final.csv')

"""# Modelling"""

df_train = pd.read_csv("/content/dataset_train_final.csv")
df_test = pd.read_csv("/content/dataset_test_final.csv")

# Convert datetime to datetime format and set as index
df_train['datetime'] = pd.to_datetime(df_train['datetime'])
df_train.set_index('datetime', inplace=True)

df_test['datetime'] = pd.to_datetime(df_test['datetime'])
df_test.set_index('datetime', inplace=True)

"""## Detail DF Train"""

df_train

df_train.columns

"""## Detail DF Test"""

df_test

df_test.columns

"""## ARIMA Model"""

# Visualize the data
plt.figure(figsize=(10, 6))
plt.plot(df_train['LeafCount'])
plt.title('LeafCount over time')
plt.xlabel('Date')
plt.ylabel('LeafCount')
plt.show()

# Check stationarity
result = adfuller(df_train['LeafCount'])
print('ADF Statistic:', result[0])
print('p-value:', result[1])
for key, value in result[4].items():
    print('Critical Values:')
    print(f'   {key}, {value}')

# Differencing to make the series stationary
df_train['LeafCount_diff'] = df_train['LeafCount'] - df_train['LeafCount'].shift(1)
df_train.dropna(inplace=True)

result = adfuller(df_train['LeafCount_diff'])
print('ADF Statistic after differencing:', result[0])
print('p-value after differencing:', result[1])

# Determine p, d, q using ACF and PACF plots
fig, ax = plt.subplots(1, 2, figsize=(16, 6))
acf_values = acf(df_train['LeafCount_diff'], nlags=20)
pacf_values = pacf(df_train['LeafCount_diff'], nlags=20)

ax[0].stem(range(len(acf_values)), acf_values, use_line_collection=True)
ax[0].set_title('ACF Plot')
ax[1].stem(range(len(pacf_values)), pacf_values, use_line_collection=True)
ax[1].set_title('PACF Plot')

plt.show()

# Fit the ARIMA model
# Replace p, d, q with appropriate values from ACF and PACF plots
p = 1  # Typically based on PACF plot
d = 1  # Differencing order
q = 1  # Typically based on ACF plot

model = ARIMA(df_train['LeafCount'], order=(1, 1, 1))
model_fit = model.fit()
print(model_fit.summary())

# Check if there are any warnings or errors during fitting
if not model_fit.mle_retvals['converged']:
    print("Warning: Model fitting did not converge properly.")

# Predict on the Test Data
start = len(df_train)
end = start + len(df_test) - 1
predictions = model_fit.predict(start=start, end=end, typ='levels')

# Calculate Evaluation Metrics (RMSE and MAE)
rmse = np.sqrt(mean_squared_error(df_test['LeafCount'], predictions))
mae = mean_absolute_error(df_test['LeafCount'], predictions)

print(f'RMSE: {rmse}')
print(f'MAE: {mae}')

forecast_result = model_fit.get_forecast(steps=30)
forecast = forecast_result.predicted_mean
conf_int = forecast_result.conf_int()

# Create a DataFrame to hold the results
forecast_dates = pd.date_range(start=df_train.index[-1] + pd.Timedelta(days=1), periods=30, freq='D')
forecast_df = pd.DataFrame({'Forecast': forecast.values, 'Lower CI': conf_int.iloc[:, 0].values, 'Upper CI': conf_int.iloc[:, 1].values}, index=forecast_dates)

# Display the forecast table
print(forecast_df)

# Plot the results
plt.figure(figsize=(12, 6))
plt.plot(df_train['LeafCount'], label='Training Data')
plt.plot(forecast_df['Forecast'], label='Forecast', color='r')
plt.fill_between(forecast_df.index, forecast_df['Lower CI'], forecast_df['Upper CI'], color='k', alpha=0.2)
plt.title('LeafCount Forecast')
plt.xlabel('Date')
plt.ylabel('LeafCount')
plt.legend()
plt.show()

"""## Prophet Model"""

df_train = pd.read_csv("/content/dataset_train_final.csv")
df_test = pd.read_csv("/content/dataset_test_final.csv")

# df_train['datetime'] = pd.to_datetime(df_train['datetime'])
# df_train.set_index('datetime', inplace=True)

# df_test['datetime'] = pd.to_datetime(df_test['datetime'])
# df_test.set_index('datetime', inplace=True)

# Convert datetime to datetime format
df_train['datetime'] = pd.to_datetime(df_train['datetime'])
df_test['datetime'] = pd.to_datetime(df_test['datetime'])

# Prepare training data for Prophet
df_train_prophet = df_train[['datetime', 'LeafCount', 'hole', 'temperature', 'humidity', 'light', 'pH', 'EC', 'TDS', 'WaterTemp']].copy()
df_train_prophet.rename(columns={'datetime': 'ds', 'LeafCount': 'y'}, inplace=True)

# Prepare test data for evaluation
df_test_prophet = df_test[['datetime', 'LeafCount', 'hole', 'temperature', 'humidity', 'light', 'pH', 'EC', 'TDS', 'WaterTemp']].copy()
df_test_prophet.rename(columns={'datetime': 'ds', 'LeafCount': 'y'}, inplace=True)

df_train_prophet

# Initialize Prophet model with additional regressors
model = Prophet()
model.add_regressor('hole')
model.add_regressor('temperature')
model.add_regressor('humidity')
model.add_regressor('light')
model.add_regressor('pH')
model.add_regressor('EC')
model.add_regressor('TDS')
model.add_regressor('WaterTemp')

# Fit the model
model.fit(df_train_prophet)

# Create future dataframe for the next 30 days
future_dates = pd.date_range(start=df_test['datetime'].max(), periods=30, freq='D')
future = pd.DataFrame({'ds': future_dates})

# Fill in the future dataframe with the last known values of the regressors
future['hole'] = df_test['hole'].iloc[-1]
future['temperature'] = df_test['temperature'].iloc[-1]
future['humidity'] = df_test['humidity'].iloc[-1]
future['light'] = df_test['light'].iloc[-1]
future['pH'] = df_test['pH'].iloc[-1]
future['EC'] = df_test['EC'].iloc[-1]
future['TDS'] = df_test['TDS'].iloc[-1]
future['WaterTemp'] = df_test['WaterTemp'].iloc[-1]

# Make predictions
forecast = model.predict(future)

# Visualize the forecast
fig = model.plot(forecast)
plt.title('Forecasted LeafCount using Prophet')
plt.xlabel('Date')
plt.ylabel('LeafCount')
plt.show()

# Create a table of the forecasted values
forecast_table = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]
forecast_table.rename(columns={'ds': 'Date', 'yhat': 'Forecasted_LeafCount', 'yhat_lower': 'Lower_Bound', 'yhat_upper': 'Upper_Bound'}, inplace=True)

print(forecast_table)

# Make predictions on the test set
test_forecast = model.predict(df_test_prophet)

# Calculate evaluation metrics
rmse = np.sqrt(mean_squared_error(df_test_prophet['y'], test_forecast['yhat']))
mae = mean_absolute_error(df_test_prophet['y'], test_forecast['yhat'])

print(f"RMSE: {rmse}")
print(f"MAE: {mae}")

"""### K-Fold Validation"""

# Function for K-Fold Cross Validation
def kfold_cross_validation(df, k=5):
    from sklearn.model_selection import KFold

    kf = KFold(n_splits=k)
    rmse_scores = []
    mae_scores = []

    for train_index, val_index in kf.split(df):
        train_data = df.iloc[train_index]
        val_data = df.iloc[val_index]

        # Initialize new Prophet model for each fold
        model = Prophet()
        model.add_regressor('hole')
        model.add_regressor('temperature')
        model.add_regressor('humidity')
        model.add_regressor('light')
        model.add_regressor('pH')
        model.add_regressor('EC')
        model.add_regressor('TDS')
        model.add_regressor('WaterTemp')

        # Fit Prophet model
        model.fit(train_data)

        # Make predictions on validation set
        val_forecast = model.predict(val_data.drop(columns=['y']))

        # Calculate evaluation metrics
        rmse = np.sqrt(mean_squared_error(val_data['y'], val_forecast['yhat']))
        mae = mean_absolute_error(val_data['y'], val_forecast['yhat'])

        rmse_scores.append(rmse)
        mae_scores.append(mae)

    # Aggregate metrics
    avg_rmse = np.mean(rmse_scores)
    avg_mae = np.mean(mae_scores)

    return avg_rmse, avg_mae

# Define the number of folds (k)
k = 5

# Perform K-Fold Cross Validation
kfold_rmse, kfold_mae = kfold_cross_validation(df_train_prophet)

print(f"Average RMSE across {k} folds: {kfold_rmse}")
print(f"Average MAE across {k} folds: {kfold_mae}")